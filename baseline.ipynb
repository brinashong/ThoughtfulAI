{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpi-wGgrIQLi"
   },
   "source": [
    "# Recurrent Convolutional Neural Networks (R-CNN) for decoding neural signals acquired from an InterAxon Muse for\n",
    "\n",
    "Dataset has already been preprocessed: [MindBigData.csv](https://drive.google.com/file/d/1S1ut3mR7poG22qZ25ngVzk8_dndgMn74/view)\n",
    "\n",
    "Original: https://colab.research.google.com/drive/1q1mvIC1xgEuPNNvgASrAG7zie3qRjr-8\n",
    "\n",
    "We are reimplementing their method in PyTorch to reproduce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12098,
     "status": "ok",
     "timestamp": 1709008964631,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "t6h5Nta_Ll-z",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math, random, torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1709008964631,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "958rKfZtMbDw",
    "outputId": "5b7567fc-dbbb-4b2c-96e6-ce67ccf1c78b"
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1709008964631,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "tTIzTW5MMd-7",
    "outputId": "712c2398-234c-4487-bdcd-147c591244b5"
   },
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4vKS2PWSLe1G"
   },
   "source": [
    "### Import dataset\n",
    "* First column indicates from which electrode, the data was obtained from (i.e., FP1, FP2, TP9, TP10).\n",
    "* Second column indicates the digit seen by test subject (ground truth label).\n",
    "* Third column onwards represent the readings obtained from the electrodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "executionInfo": {
     "elapsed": 552,
     "status": "ok",
     "timestamp": 1709009074835,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "M8BuxLXpHfjh",
    "outputId": "9b47b6b1-f240-4771-ab9c-5e3c975aa641"
   },
   "outputs": [],
   "source": [
    "full_data = pd.read_csv(\"MindBigData.csv\", header = None) ### code did not include first row of data, but I included here\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1709009061239,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "oiza_hn-V_tK",
    "outputId": "d483dda4-5c51-408a-faed-8521cf6ca66c"
   },
   "outputs": [],
   "source": [
    "full_data[0].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "error",
     "timestamp": 1709008979180,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "q9lhN5xRNshb",
    "outputId": "da64137d-26ca-4d21-8611-67e682b72299"
   },
   "outputs": [],
   "source": [
    "full_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ew5JmjDYN__I"
   },
   "source": [
    "There are 408 features (for each EGG reading). Hmmm never distinguish FP1, FP2, TP9, TP10 for now..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T3HH8lJN4no"
   },
   "source": [
    "### Model training & testing\n",
    "WEIRD that originally here is how the data was obtained:\n",
    "``` python\n",
    "# where labels are {FP1, FP2, TP9, TP10}\n",
    "# the data features are the EEG readings tgt with the digits seen by the test subject\n",
    "data = full_data.iloc[:, 1:].values\n",
    "labels = full_data.iloc[:, 0].values\n",
    "```\n",
    "I am unable to reproduce the result even when using their original code, model isn't learning at all.\n",
    "\n",
    "Hence, I decided to vary the data used:\n",
    "``` python\n",
    "# where labels are {0,1,...,8,9}, I dropped -1\n",
    "# the data features are the EEG readings (FP1, FP2, TP9, TP10 treated as input channels)\n",
    "full_data = full_data[full_data[1]!=-1]\n",
    "full_data_channels = np.array([full_data[full_data[0] == channel].iloc[:, 1:].values for channel in [\"FP1\", \"FP2\", \"TP9\", \"TP10\"]])\n",
    "full_data_channels = full_data_channels.reshape((full_data_channels.shape[1], full_data_channels.shape[0], full_data_channels.shape[2]))\n",
    "data = full_data_channels[:,:,1:]\n",
    "labels = full_data_channels[:,0,0]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1nCPtZ2xtmWR"
   },
   "source": [
    "##### PyTorch implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1709009094861,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "YEOWzLFYVoud"
   },
   "outputs": [],
   "source": [
    "# Prepare dataset\n",
    "full_data = full_data[full_data[1]!=-1]\n",
    "full_data_channels = np.array([full_data[full_data[0] == channel].iloc[:, 1:].values for channel in [\"FP1\", \"FP2\", \"TP9\", \"TP10\"]])\n",
    "full_data_channels = full_data_channels.reshape((full_data_channels.shape[1], full_data_channels.shape[0], full_data_channels.shape[2]))\n",
    "data = full_data_channels[:,:,1:]\n",
    "labels = full_data_channels[:,0,0]\n",
    "\n",
    "print(f\"Raw number of labels:\\n{pd.Series(labels).value_counts()}\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "one_hot_labels = to_categorical(encoded_labels)\n",
    "\n",
    "# Split the data into training, testing, and validation sets\n",
    "# (nv followed his one, only used a few for training for testing)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(data, one_hot_labels, test_size=0.8, random_state=42, stratify = one_hot_labels)\n",
    "print(f\"\\nSize of X_train:\\n{X_train.shape}\")\n",
    "print(f\"\\nSize of y_train:\\n{y_train.shape}\")\n",
    "# X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify = y_temp)\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"Custom dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        out = {'data': self.data[idx], 'label': self.label[idx]}\n",
    "        return out\n",
    "\n",
    "train_loader = DataLoader(MyDataset(X_train, y_train), batch_size=128, shuffle=True) ###\n",
    "\n",
    "# Create model\n",
    "class RCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RCNN,self).__init__()\n",
    "        # 4 x 408\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 4, out_channels = 64, kernel_size = 3, padding = 0, stride = 1),\n",
    "            # 64 x 406\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2)\n",
    "            # 64 x 203\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(in_channels = 64, out_channels = 128, kernel_size = 3, padding = 0, stride = 1),\n",
    "            # 128 x 201\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(in_channels = 128, out_channels = 128, kernel_size = 6, padding = 0, stride = 1),\n",
    "            # 128 x 196\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size = 2)\n",
    "            # 128 x 98\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size = 128*98, hidden_size = 80) ### 128*98\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(80,80), ###\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(80,10) ###\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, x.shape[-1]*x.shape[-2]) # flatten\n",
    "        x = self.lstm(x)[0]\n",
    "        x = self.dense(x)\n",
    "        # x = torch.nn.functional.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1709009099220,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "w7Zv4hchqUP-",
    "outputId": "1346dfb3-c330-491a-9814-93c1d8884951"
   },
   "outputs": [],
   "source": [
    "# check trainable parameters\n",
    "for p in RCNN().parameters():\n",
    "    print(p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 624,
     "status": "ok",
     "timestamp": 1709009103779,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "JozHweFIbhM_",
    "outputId": "d8db3b65-2466-4d34-f664-feed9982a4ac"
   },
   "outputs": [],
   "source": [
    "# test output shape\n",
    "network = RCNN()\n",
    "# convert to tensor\n",
    "x = torch.from_numpy(X_train[0:1])\n",
    "x = x.type(torch.float)\n",
    "print(x.shape)\n",
    "preds = network(x)\n",
    "print(preds.shape)\n",
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "executionInfo": {
     "elapsed": 1622,
     "status": "error",
     "timestamp": 1709009196357,
     "user": {
      "displayName": "YeeYing Tan",
      "userId": "02040042571250191470"
     },
     "user_tz": -480
    },
    "id": "c8vr_x-stkOI",
    "outputId": "2f7737b2-23ea-4798-f6ea-04bfa7065dea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training\n",
    "def one_hot_ce_loss(outputs, targets):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    _, labels = torch.max(targets, dim=1)\n",
    "    return criterion(outputs, labels)\n",
    "\n",
    "network = RCNN()\n",
    "if torch.cuda.is_available():\n",
    "    network = network.cuda(1)\n",
    "\n",
    "optimizer = optim.Adam(network.parameters(), lr=0.00001)\n",
    "\n",
    "for epoch in range(25):\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    for batch in train_loader:\n",
    "        data, labels = batch[\"data\"], batch[\"label\"]\n",
    "        data = data.type(torch.float)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda(1)\n",
    "            labels = labels.cuda(1)\n",
    "\n",
    "        preds = network(data)\n",
    "        loss = one_hot_ce_loss(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _,prelabels=torch.max(preds,dim=1)\n",
    "        total_correct += (torch.max(labels, dim=1)[1] == torch.max(preds, dim=1)[1]).sum().item()\n",
    "    accuracy = total_correct/len(X_train)\n",
    "    print(\"Epoch:%d  ,  Loss:%f  , Train Accuracy:%f \"%(epoch, total_loss, accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see last batch of predictions and labels\n",
    "torch.max(labels, dim=1)[1], torch.max(preds, dim=1)[1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOorPWaBnjcDnplPjfHlawc",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
